{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contents:\n",
    "1. [Background](#Background)\n",
    "2. [Environment Setup](#Environment-Setup)\n",
    "    - [Install the required python packages](#Install-the-required-python-packages)\n",
    "    - [Import the required libraries](#Import-the-required-libraries)\n",
    "3. [Download and prepare the data](#Download-and-prepare-the-data)\n",
    "4. [Scenario 1: Related time series are available in the forecast horizon](#Scenario-1:-Related-time-series-are-available-in-the-forecast-horizon)\n",
    "5. [Scenario 2: Related time series is not available in the forecast horizon](#Scenario-2:-Related-time-series-is-not-available-in-the-forecast-horizon)\n",
    "6. [Scenario 3: Model all the time series as target series](#Model-all-the-time-series-as-target-series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background\n",
    "\n",
    "Multivariate time series forecasting is a common problem and more recently deep learning models have been applied to time series forecasting. [GluonTS](https://ts.gluon.ai/index.html) is a deep learning toolkit for probabilistic modelling of time series. This notebook shows you different ways in which one can model a multivariate time series problem (time series with related variables) using different models that are implemented in GluonTS.\n",
    "The following models are explored in this notebook -\n",
    "- [DeepAR](https://ts.gluon.ai/api/gluonts/gluonts.model.deepar.html)\n",
    "- [Transformer](https://ts.gluon.ai/api/gluonts/gluonts.model.transformer.html)\n",
    "- [MQ-CNN](https://ts.gluon.ai/api/gluonts/gluonts.model.seq2seq.html)\n",
    "- [Temporal Fusion Transformer](https://ts.gluon.ai/api/gluonts/gluonts.model.tft.html)\n",
    "- [LSTNet](https://ts.gluon.ai/api/gluonts/gluonts.model.lstnet.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "\n",
    "Please run this notebook on an instance that has a GPU. (p2.xlarge or higher)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Install the required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade mxnet~=1.7\n",
    "!pip install gluonts\n",
    "!pip install mxnet-cu102==1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gluonts.dataset.common import (\n",
    "    CategoricalFeatureInfo,\n",
    "    ListDataset,\n",
    "    MetaData,\n",
    "    TrainDatasets,\n",
    "    load_datasets\n",
    ")\n",
    "\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.model.transformer import TransformerEstimator\n",
    "from gluonts.model.lstnet import LSTNetEstimator\n",
    "from gluonts.model.seq2seq import MQCNNEstimator\n",
    "from gluonts.model.seq2seq import MQRNNEstimator\n",
    "from gluonts.model.tft import TemporalFusionTransformerEstimator\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import Evaluator, MultivariateEvaluator\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
    "\n",
    "import mxnet as mx\n",
    "\n",
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download and prepare the data\n",
    "\n",
    "We use the [PM 2.5 dataset](https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data) from the UCI Machine Learning Repository.\n",
    "\n",
    "The dataset contains PM 2.5 data from the US Embassy in Beijing and is supplemented with meteorological data. The following columns are part of the data - \n",
    "- No: row number\n",
    "- year: year of data in this row\n",
    "- month: month of data in this row\n",
    "- day: day of data in this row\n",
    "- hour: hour of data in this row\n",
    "- pm2.5: PM2.5 concentration (ug/m^3)\n",
    "- DEWP: Dew Point (â„ƒ)\n",
    "- TEMP: Temperature (â„ƒ)\n",
    "- PRES: Pressure (hPa)\n",
    "- cbwd: Combined wind direction\n",
    "- Iws: Cumulated wind speed (m/s)\n",
    "- Is: Cumulated hours of snow\n",
    "- Ir: Cumulated hours of rain\n",
    "\n",
    "Given the above information, here is how the different features in the dataset are treated - \n",
    "- pm2.5 is the target variable. \n",
    "- Meteorological variables like 'TEMP', 'DEWP' and 'PRES' can be treated as related time series with real values.\n",
    "- 'cbwd' is a categorical variable and varies with time and can be treated as a dynamic categorical feature.\n",
    "\n",
    "There are different ways in which one can model multivariate time series problems depending on the availability of related time series features in the forecast horizon. This notebook illustrates them by assuming the presence or absence of the meteorological variables in the forecast horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00381/PRSA_data_2010.1.1-2014.12.31.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"PRSA_data_2010.1.1-2014.12.31.csv\")\n",
    "\n",
    "#combine month,day,hour into a timestamp\n",
    "df['Timestamp'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']])\n",
    "\n",
    "#set an ID to identify a time series\n",
    "df['id'] = 0\n",
    "\n",
    "#set the type of the categorical variable\n",
    "df[\"cbwd\"] = df[\"cbwd\"].astype('category')\n",
    "df[\"cbwd_cat\"] = df[\"cbwd\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Related time series are available in the forecast horizon\n",
    "\n",
    "In this section, you will assume that the meteorological variables (TEMP, DEWP, PRES) are available to the model in the forecast horizon. In real life, this could be from a weather prediction model or forecast.\n",
    "\n",
    "The following cells compare a DeepAR and Transformer in this particular scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Prepare the training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_length = 120\n",
    "num_backtest_windows = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_list = []\n",
    "test_data_list = []\n",
    "\n",
    "for i in reversed(range(1, num_backtest_windows+1)):\n",
    "      \n",
    "    training_data = [\n",
    "        {\n",
    "            \"start\": df.iloc[0][\"Timestamp\"],\n",
    "            \"target\": df[\"pm2.5\"][:-forecast_length*i],\n",
    "            \"feat_static_cat\": [0],\n",
    "            \"feat_dynamic_real\": [df[\"TEMP\"][:-forecast_length*i],\n",
    "                                 df[\"DEWP\"][:-forecast_length*i]],\n",
    "            \"feat_dynamic_cat\": [df[\"cbwd_cat\"][:-forecast_length*i]]\n",
    "        }\n",
    "        ]\n",
    "    \n",
    "    # create testing data.\n",
    "    test_data = [\n",
    "        {\n",
    "            \"start\": df.iloc[0][\"Timestamp\"],\n",
    "            \"target\": df[\"pm2.5\"][:-forecast_length*(i-1)] if i>1 else df[\"pm2.5\"][:],\n",
    "            \"feat_static_cat\": [0],\n",
    "            \"feat_dynamic_real\": [df[\"TEMP\"][:-forecast_length*(i-1)] if i>1 else df[\"TEMP\"][:],\n",
    "                                 df[\"DEWP\"][:-forecast_length*(i-1)] if i>1 else df[\"DEWP\"][:]],\n",
    "            \"feat_dynamic_cat\": [df[\"cbwd_cat\"][:-forecast_length*(i-1)] if i>1 else df[\"cbwd_cat\"][:]]\n",
    "        }\n",
    "        ]\n",
    "\n",
    "    training_data_list.append(ListDataset(training_data, freq='1h'))\n",
    "    test_data_list.append(ListDataset(test_data, freq='1h'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that takes a gluonTS estimator, trains the model and predicts it for every pair of train and test dataset\n",
    "def backtest(model,\n",
    "             training_data_list,\n",
    "             test_data_list,\n",
    "             num_backtest_windows):\n",
    "    \n",
    "    forecasts = []\n",
    "    obs = []\n",
    "    \n",
    "    #train a model for every backtest window\n",
    "    for i in range(num_backtest_windows):\n",
    "        predictor = model.train(training_data_list[i],\n",
    "                               force_reinit=True)\n",
    "        forecast_it, ts_it = make_evaluation_predictions(test_data_list[i], \n",
    "                                                         predictor=predictor, \n",
    "                                                         num_samples=100)\n",
    "        forecasts.extend(list(forecast_it))\n",
    "        obs.extend(list(ts_it))\n",
    "    return forecasts, obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  DeepAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "deepar = DeepAREstimator(freq=\"1h\",\n",
    "                         use_feat_static_cat=True,\n",
    "                         use_feat_dynamic_real=True,\n",
    "                         cardinality=[1],\n",
    "                         prediction_length=forecast_length,\n",
    "                         trainer=Trainer(epochs=30, ctx = mx.context.gpu(0)),\n",
    "                         num_cells=40)\n",
    "forecast_deepar, obs_deepar = backtest(deepar,\n",
    "                                       training_data_list,\n",
    "                                       test_data_list,\n",
    "                                       num_backtest_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "transformer = TransformerEstimator(freq=\"1h\",\n",
    "                                   use_feat_dynamic_real=True,\n",
    "                                   #context_length=168,\n",
    "                                   prediction_length=forecast_length,\n",
    "                                   trainer=Trainer(epochs=10,\n",
    "                                                   learning_rate=0.01,\n",
    "                                                   ctx = mx.context.gpu(0))\n",
    "                                  )\n",
    "forecast_transformer, obs_transformer = backtest(transformer,\n",
    "                                                 training_data_list,\n",
    "                                                 test_data_list,\n",
    "                                                 num_backtest_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(quantiles=[0.5], seasonality=None)\n",
    "agg_metrics_deepar, item_metrics_deepar = evaluator(iter(obs_deepar), \n",
    "                                                        iter(forecast_deepar), \n",
    "                                                        num_series=len(forecast_deepar))\n",
    "\n",
    "evaluator = Evaluator(quantiles=[0.5], seasonality=None)\n",
    "agg_metrics_transformer, item_metrics_transformer = evaluator(iter(obs_transformer),\n",
    "                                                              iter(forecast_transformer),\n",
    "                                                              num_series=len(forecast_transformer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame.from_dict(agg_metrics_deepar, orient='index').rename(columns={0: \"DeepAR\"}),\n",
    "           pd.DataFrame.from_dict(agg_metrics_transformer, orient='index').rename(columns={0: \"Transformer\"})],\n",
    "         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecasts(obs, forecasts, past_length, start, stop, step, title):\n",
    "    for target, forecast in zip(obs, forecasts):\n",
    "        ax = target[-past_length:].plot(figsize=(12, 5), linewidth=2)\n",
    "        forecast.plot(color='g')\n",
    "        plt.ylabel('PM2.5 concentration (ug/m^3)')\n",
    "        plt.title(title)\n",
    "        plt.grid(which='both')\n",
    "        plt.legend([\"observations\", \"median prediction\", \"90% confidence interval\", \"50% confidence interval\"])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below charts plot the observed PM2.5 against the forecast from DeepAR and Transformer. Since, the model computes probabilistic forecasts, it is possible to draw a confidence interval around the median prediction. These charts show a 50% and 90% confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Sample Forecast - DeepAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecasts(obs_deepar, forecast_deepar, 340, 0, 2, 1, 'deepar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Sample Forecast - Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecasts(obs_transformer, forecast_transformer, 340, 0, 2, 1, 'transformer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Related time series is not available in the forecast horizon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will see how to train models when the related time series (meteorological features in this case) is not available in the forecast horizon. The meteorological variables are only present for the historical time period and can hence be used for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq models like [MQ-CNN](https://ts.gluon.ai/api/gluonts/gluonts.model.seq2seq.html) which uses a CNN as an encoder and a MLP as a decoder can be used in this scenario.\n",
    "\n",
    "[Temporal Fusion Transformer](https://arxiv.org/abs/1912.09363) is another architecture that combines recurrent layers and attention layers to enable the usage of a mix of inputs like exogenous variables that are only observed historically and other static and dynamic covariates.\n",
    "\n",
    "We compare the above to models in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Prepare the training and testing dataset to use 'past_feat_dynamic_real'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_list = []\n",
    "test_data_list = []\n",
    "\n",
    "for i in reversed(range(1, 3)):\n",
    "      \n",
    "    training_data = [\n",
    "        {\n",
    "            \"start\": df.iloc[0][\"Timestamp\"],\n",
    "            \"target\": df[\"pm2.5\"][:-forecast_length*i],\n",
    "            \"past_feat_dynamic_real\": [df[\"TEMP\"][:-forecast_length*i],\n",
    "                                 df[\"DEWP\"][:-forecast_length*i]],\n",
    "        }\n",
    "        ]\n",
    "    \n",
    "    # create testing data.\n",
    "    test_data = [\n",
    "        {\n",
    "            \"start\": df.iloc[0][\"Timestamp\"],\n",
    "            \"target\": df[\"pm2.5\"][:-forecast_length*(i-1)] if i>1 else df[\"pm2.5\"][:],\n",
    "            \"past_feat_dynamic_real\": [df[\"TEMP\"][:-forecast_length*(i-1)] if i>1 else df[\"TEMP\"][:],\n",
    "                                 df[\"DEWP\"][:-forecast_length*(i-1)] if i>1 else df[\"DEWP\"][:]],\n",
    "        }\n",
    "        ]\n",
    "\n",
    "    training_data_list.append(ListDataset(training_data, freq='1h'))\n",
    "    test_data_list.append(ListDataset(test_data, freq='1h'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MQ-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At times, one can encounter exploding gradients and as a result the loss can become a NaN. \n",
    "#set hybridize=False. May be related to https://github.com/awslabs/gluon-ts/issues/833\n",
    "mqcnn = MQCNNEstimator(freq=\"1h\",\n",
    "                       use_past_feat_dynamic_real=True,\n",
    "                       prediction_length=forecast_length,\n",
    "                       trainer=Trainer(epochs=30,\n",
    "                                       learning_rate=0.001,\n",
    "                                       #clip_gradient=3,\n",
    "                                       #batch_size=32,\n",
    "                                       #num_batches_per_epoch=16,\n",
    "                                       hybridize=False,\n",
    "                                       ctx = mx.context.gpu(0)),\n",
    "                       \n",
    "                      )\n",
    "forecast_mqcnn, obs_mqcnn = backtest(mqcnn,\n",
    "                                   training_data_list,\n",
    "                                   test_data_list,\n",
    "                                   num_backtest_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal Fusion Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_list = []\n",
    "test_data_list = []\n",
    "\n",
    "for i in reversed(range(1, 3)):\n",
    "      \n",
    "    training_data = [\n",
    "        {\n",
    "            \"start\": df.iloc[0][\"Timestamp\"],\n",
    "            \"target\": df[\"pm2.5\"][:-forecast_length*i],\n",
    "            \"past_feat_dynamic_real_1\": df[\"TEMP\"][:-forecast_length*i],\n",
    "            \"past_feat_dynamic_real_2\": df[\"DEWP\"][:-forecast_length*i],\n",
    "            \"past_feat_dynamic_real_3\": df[\"Ir\"][:-forecast_length*i]\n",
    "        }\n",
    "        ]\n",
    "    \n",
    "    # create testing data.\n",
    "    test_data = [\n",
    "        {\n",
    "            \"start\": df.iloc[0][\"Timestamp\"],\n",
    "            \"target\": df[\"pm2.5\"][:-forecast_length*(i-1)] if i>1 else df[\"pm2.5\"][:],\n",
    "            \"past_feat_dynamic_real_1\": df[\"TEMP\"][:-forecast_length*(i-1)] if i>1 else df[\"TEMP\"][:],\n",
    "            \"past_feat_dynamic_real_2\": df[\"DEWP\"][:-forecast_length*(i-1)] if i>1 else df[\"DEWP\"][:],\n",
    "            \"past_feat_dynamic_real_3\": df[\"Ir\"][:-forecast_length*(i-1)] if i>1 else df[\"Ir\"][:]\n",
    "        }\n",
    "        ]\n",
    "\n",
    "    training_data_list.append(ListDataset(training_data, freq='1h'))\n",
    "    test_data_list.append(ListDataset(test_data, freq='1h'))\n",
    "\n",
    "feat_past_dynamic_real = [\"past_feat_dynamic_real_1\", \"past_feat_dynamic_real_2\", \"past_feat_dynamic_real_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/awslabs/gluon-ts/issues/1075\n",
    "tft = TemporalFusionTransformerEstimator(freq = '1h',\n",
    "                                         context_length=168,\n",
    "                                         prediction_length = forecast_length,\n",
    "                                         trainer=Trainer(epochs=30,\n",
    "                                                         learning_rate=0.001,\n",
    "                                                         ctx = mx.context.gpu(0)),\n",
    "                                         hidden_dim=32,\n",
    "                                         variable_dim=8,\n",
    "                                         num_heads=4,\n",
    "                                         num_outputs=3,\n",
    "                                         num_instance_per_series=100,\n",
    "                                         dropout_rate=0.1,\n",
    "                                         dynamic_feature_dims={\n",
    "                                             'past_feat_dynamic_real_1': 1,\n",
    "                                             'past_feat_dynamic_real_2': 1,\n",
    "                                             'past_feat_dynamic_real_3': 1\n",
    "                                         }, # dimensions of dynamic real features\n",
    "                                         past_dynamic_features=feat_past_dynamic_real,\n",
    "                                        )\n",
    "forecast_tft, obs_tft = backtest(tft,\n",
    "                                 training_data_list,\n",
    "                                 test_data_list,\n",
    "                                 num_backtest_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(quantiles=[0.5], seasonality=None)\n",
    "agg_metrics_mqcnn, item_metrics_mqcnn = evaluator(iter(obs_mqcnn), \n",
    "                                                    iter(forecast_mqcnn), \n",
    "                                                    num_series=len(forecast_mqcnn))\n",
    "\n",
    "evaluator = Evaluator(quantiles=[0.5], seasonality=None)\n",
    "agg_metrics_tft, item_metrics_tft = evaluator(iter(obs_tft),\n",
    "                                                  iter(forecast_tft),\n",
    "                                                  num_series=len(forecast_tft))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame.from_dict(agg_metrics_mqcnn, orient='index').rename(columns={0: \"MQ-CNN\"}),\n",
    "           pd.DataFrame.from_dict(agg_metrics_tft, orient='index').rename(columns={0: \"TFT\"})],\n",
    "         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'QuantileForecast.plot' plots all the quantiles as line plots.\n",
    "# This is some boiler plate code to plot an interval around the median \n",
    "# using the 10th and 90th quantile\n",
    "def plot_from_quantile_forecast(obs, past_length, lower_bound, upper_bound, forecasts):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(obs[0][-forecast_length-past_length:], label='observed')\n",
    "    plt.plot(obs[0][-forecast_length:].index, \n",
    "             lower_bound,\n",
    "            color='g',\n",
    "            alpha=0.3,\n",
    "            label='10th quantile')\n",
    "    plt.plot(obs[0][-forecast_length:].index, \n",
    "             forecasts,\n",
    "            color='g',\n",
    "            label='median prediction')\n",
    "    plt.plot(obs[0][-forecast_length:].index, \n",
    "             upper_bound,\n",
    "            color='g',\n",
    "            alpha=0.3,\n",
    "            label='90th quantile')\n",
    "    plt.fill_between(obs[0][-forecast_length:].index,\n",
    "                     lower_bound, \n",
    "                     upper_bound,\n",
    "                    color='g',\n",
    "                    alpha=0.3)\n",
    "    plt.ylabel('PM2.5 concentration (ug/m^3)')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(which=\"both\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two models illustrated in this section forecast quantiles. Hence to construct an interval, one needs to pick forecasts at different quantiles. The charts below use the 10th and the 90th quantile forecast to construct an interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Sample Forecast - MQCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_from_quantile_forecast(obs_mqcnn, \n",
    "                            100, \n",
    "                            forecast_mqcnn[0].forecast_array[0], \n",
    "                            forecast_mqcnn[0].forecast_array[8],\n",
    "                            forecast_mqcnn[0].forecast_array[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Sample Forecast - Temporal Fusion Transformer (TFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_from_quantile_forecast(obs_tft, \n",
    "                            100, \n",
    "                            forecast_tft[0].forecast_array[1], \n",
    "                            forecast_tft[0].forecast_array[2],\n",
    "                            forecast_tft[0].forecast_array[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model all the time series as target series\n",
    "\n",
    "In this case, we forecast pm2.5 and the other meteorological features together as multivariate variables.\n",
    "\n",
    "Models like [LSTNet](https://ts.gluon.ai/api/gluonts/gluonts.model.lstnet.html) allow one to treat all the related time series in a multivariate fashion. One can train a model to forecast all the time series simultaneously.\n",
    "\n",
    "For this, the data needs to be prepared in a different way and the below cell does that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train = df.transpose()\n",
    "train2 = train.to_numpy()\n",
    "\n",
    "target=train2[[5,6,7,8],:]\n",
    "\n",
    "#prediction_length=24\n",
    "\n",
    "start= [df.iloc[0][\"Timestamp\"] for _ in range(4)]\n",
    "\n",
    "\n",
    "train_ds = ListDataset([{FieldName.TARGET: target, \n",
    "                         FieldName.START: start\n",
    "                         } \n",
    "                        for (target, start) in zip(target[:, :-forecast_length], \n",
    "                                                            start)],\n",
    "                      freq='1h')\n",
    "\n",
    "test_ds = ListDataset([{FieldName.TARGET: target, \n",
    "                        FieldName.START: start\n",
    "                       }\n",
    "                       for (target, start) in zip(target[:, :],\n",
    "                                                  start)],\n",
    "                      freq='1h')\n",
    "\n",
    "\n",
    "lstnet_estimator=LSTNetEstimator(freq='1h', \n",
    "                          prediction_length=forecast_length, \n",
    "                          context_length=336, \n",
    "                          num_series=4, \n",
    "                          skip_size=10, \n",
    "                          ar_window=320, \n",
    "                          channels=80, \n",
    "                          trainer = Trainer(epochs=400,\n",
    "                                            ctx = mx.context.gpu(0)), \n",
    "                          dropout_rate = 0.4, \n",
    "                          output_activation = 'sigmoid', \n",
    "                          rnn_cell_type = 'gru', \n",
    "                          rnn_num_cells = 100, \n",
    "                          rnn_num_layers = 6, \n",
    "                          skip_rnn_cell_type = 'gru', \n",
    "                          skip_rnn_num_layers = 3, \n",
    "                          skip_rnn_num_cells = 10, \n",
    "                          scaling = True)\n",
    "\n",
    "\n",
    "grouper_train = MultivariateGrouper(max_target_dim=4)\n",
    "\n",
    "train_ds = grouper_train(train_ds)\n",
    "\n",
    "lstnet_predictor = lstnet_estimator.train(train_ds)\n",
    "\n",
    "grouper_test = MultivariateGrouper(max_target_dim=4)\n",
    "\n",
    "test_ds = grouper_test(test_ds)\n",
    "\n",
    "forecast_lstnet, obs_lstnet = make_evaluation_predictions(test_ds,\n",
    "                                                          predictor=lstnet_predictor,\n",
    "                                                         num_samples=100)\n",
    "\n",
    "forecast_lstnet = list(forecast_lstnet)\n",
    "obs_lstnet = list(obs_lstnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MultivariateEvaluator(quantiles=[0.1, 0.5, 0.9])\n",
    "agg_metrics_lstnet, item_metrics_lstnet = evaluator(obs_lstnet, forecast_lstnet, num_series=len(test_ds))\n",
    "\n",
    "index_series_map = {0: \"pm 2.5\",\n",
    "                   1: \"DEWP\",\n",
    "                   2: \"TEMP\",\n",
    "                   3: \"PRES\"}\n",
    "metrics_lstnet = []\n",
    "for i in range(4):\n",
    "    metrics = [k for k in agg_metrics_lstnet.keys() if k.startswith(str(i))]\n",
    "    metrics_lstnet.append(pd.DataFrame.from_dict({m[2:]:agg_metrics_lstnet[m] for m in metrics},\n",
    "                                                 orient='index').rename(columns={0: index_series_map[i]}))\n",
    "pd.concat(metrics_lstnet, axis=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Sample Forecast - LSTNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below plots show the forecasts for each of the target time series as defined in the above cell. The results from the LSTNet model that is trained to forecast all the time series simultaneously is not great. One, probably needs to do a more thorough hyperparameter optimization. But this can be a good model to explore when one wants to build a single model to forecast multiple time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x, y  in zip(obs_lstnet, forecast_lstnet):\n",
    "    for i in range(4):\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(x[i][-forecast_length-100:])\n",
    "        median = y.copy_dim(i).quantile(0.5)\n",
    "        y_10 = y.copy_dim(i).quantile(0.1)\n",
    "        y_90 = y.copy_dim(i).quantile(0.9)\n",
    "        #print(y_10)\n",
    "        #print(y_90)\n",
    "        plt.plot(x[i][-forecast_length:].index,\n",
    "                 median,\n",
    "                color='g')\n",
    "        plt.fill_between(x[i][-forecast_length:].index,\n",
    "                         y_10,\n",
    "                        y_90,\n",
    "                        color='g',\n",
    "                        alpha=0.3)\n",
    "        plt.title(index_series_map[i])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, this notebook illustrates how one can use different deep learning models that are defined in [GluonTS](https://ts.gluon.ai/index.html). The choice of models depends on the nature of the covariates and exogenous variables that are present in the dataset. GluonTS provides a rich variety of recent deep learning based models for modelling time series and this notebook can be used to quickly benchmark some of them so that one can shortlist a couple for further experimentation and fine-tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
